{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKc1siZPLNoZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import copy\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import re\n",
        "import torch\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#import spacy\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas(desc='Progress')\n",
        "from collections import Counter\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "# cross validation and metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from multiprocessing import  Pool\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "df=pd.read_csv(\"/content/train.csv\")\n",
        "# Remove column name 'text_id'\n",
        "df.drop(df.columns[[0]], axis=1, inplace=True)\n",
        "df.head()\n",
        "df.info()\n",
        "def process_data(df , column):\n",
        "    # Remove punctuation\n",
        "    punct = set(string.punctuation)\n",
        "    # tokens = [token for token in tokens if token not in punct]\n",
        "    df[column]=df[column].apply(lambda x: ''.join(ch for ch in x if ch not in punct))\n",
        "\n",
        "    #Remove special characters\n",
        "    df[column] = df[column].str.replace('[^\\w\\s]',' ')\n",
        "\n",
        "    # Lowercase\n",
        "    # tokens = [token.lower() for token in tokens]\n",
        "    df[column] = df[column].apply(lambda x: x.lower())\n",
        "\n",
        "    # Remove \\n\n",
        "    # tokens = [token.replace('\\n',' ') for token in tokens]\n",
        "    df[column] = df[column].apply(lambda x: x.replace('\\n',''))\n",
        "\n",
        "    # Remove ordinal number patterns\n",
        "    df[column] = df[column].str.replace(r'\\d+(?:st|nd|rd|th)', '')\n",
        "\n",
        "    # Remove all numbers from text\n",
        "    # Clean numbers from text column\n",
        "    df[column] = df[column].str.replace('\\d+', '')\n",
        "\n",
        "    #Remove numbers\n",
        "    from string import digits\n",
        "    remove_digits = str.maketrans('','',digits)\n",
        "    df[column] = df[column].apply(lambda x: x.translate(remove_digits))\n",
        "\n",
        "    #Remove extra space\n",
        "    import re\n",
        "    df[column] = df[column].apply(lambda x: x.strip())\n",
        "    df[column] = df[column].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "\n",
        "    return(df)\n",
        "df = process_data(df, 'sentence')\n",
        "df.head()\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word,pos='v') for word in words]\n",
        "    return ' '.join(words)\n",
        "df['sentence'] = df['sentence'].apply(lemmatize_words)\n",
        "df.head()\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "train_df, val_df = train_test_split(df, test_size=0.20)\n",
        "print(\"Train shape : \",train_df.shape)\n",
        "print(\"Test shape : \",val_df.shape)\n",
        "train_X=train_df['sentence']\n",
        "max_features = 50000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_X))\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "val_X = tokenizer.texts_to_sequences(val_df['sentence'])\n",
        "maxlen = 180\n",
        "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
        "train_df['neutral']=train_df['gold_label']==0\n",
        "train_df['negative']=train_df['gold_label']==-1\n",
        "train_df['positive']=train_df['gold_label']==1\n",
        "train_df['neutral']=train_df['neutral'].astype(float)\n",
        "train_df['negative']=train_df['negative'].astype(float)\n",
        "train_df['positive']=train_df['positive'].astype(float)\n",
        "val_df['neutral']=val_df['gold_label']==0\n",
        "val_df['negative']=val_df['gold_label']==-1\n",
        "val_df['positive']=val_df['gold_label']==1\n",
        "val_df['neutral']=val_df['neutral'].astype(float)\n",
        "val_df['negative']=val_df['negative'].astype(float)\n",
        "val_df['positive']=val_df['positive'].astype(float)\n",
        "train_df.head()\n",
        "train_y = (train_df[['negative','neutral','positive']].values)\n",
        "val_y = (val_df[['negative','neutral','positive']].values)\n",
        "embed_size = 128 # how big is each word vector\n",
        "batch_size = 128 # how many samples to process at once\n",
        "n_splits = 4 # Number of K-fold Splits\n",
        "SEED = 123\n",
        "debug = 0\n",
        "class BiLSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, num_layers, n_classes, dropout=0.1, pretrained_embeddings=None):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        # self.bilstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.bilstm1 = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.bilstm2 = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.bilstm1(x)\n",
        "        output, _ = self.bilstm2(output)\n",
        "        output = self.dropout(output)\n",
        "        output = torch.mean(output, dim=1)\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc1(output)\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc2(output)\n",
        "        return output\n",
        "# class LSTMModel(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden_size, num_layers, n_classes, dropout=0.1):\n",
        "#         super(LSTMModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(max_features, embed_size)\n",
        "#         self.rnn = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=False)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.fc1 = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         output, _ = self.rnn(x)\n",
        "#         output = self.dropout(output)\n",
        "#         output = torch.mean(output, dim=1)\n",
        "#         output = self.dropout(output)\n",
        "#         output = self.fc1(output)\n",
        "#         return output\n",
        "\n",
        "# class ImprovedRNN(nn.Module):\n",
        "\n",
        "#     def __init__(self, hidden_size, num_layers, n_classes, dropout=0.1):\n",
        "#         super(ImprovedRNN, self).__init__()\n",
        "#         self.embedding = nn.Embedding(max_features, embed_size)\n",
        "#         self.rnn = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "#         self.attention = nn.MultiheadAttention(embed_dim=hidden_size*2, num_heads=8)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.fc1 = nn.Linear(hidden_size*2, n_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         output, _ = self.rnn(x)\n",
        "#         output = self.dropout(output)\n",
        "#         output, _ = self.attention(output.transpose(0, 1), output.transpose(0, 1), output.transpose(0, 1))\n",
        "#         output = output.transpose(0, 1)\n",
        "#         output = torch.mean(output, dim=1)\n",
        "#         output = self.dropout(output)\n",
        "#         output = self.fc1(output)\n",
        "#         return output\n",
        "\n",
        "# class RNN_Text(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(RNN_Text, self).__init__()\n",
        "#         hidden_size = 64\n",
        "#         num_layers = 4\n",
        "#         n_classes = 3\n",
        "#         self.embedding = nn.Embedding(max_features, embed_size)\n",
        "#         self.rnn = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "#         self.fc1 = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         output, hidden = self.rnn(x)\n",
        "#         x = output[:, -1, :]  # take the final hidden state\n",
        "#         x = self.fc1(x)\n",
        "#         return x\n",
        "\n",
        "# class CNN_Text(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(CNN_Text, self).__init__()\n",
        "#         filter_sizes = [1,2,3,5]\n",
        "#         num_filters = 40\n",
        "#         n_classes = 3\n",
        "#         self.embedding = nn.Embedding(max_features, embed_size)\n",
        "#         self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n",
        "#         self.dropout = nn.Dropout(0.1)\n",
        "#         self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x = x.unsqueeze(1)\n",
        "#         x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n",
        "#         x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "#         x = torch.cat(x, 1)\n",
        "#         x = self.dropout(x)\n",
        "#         logit = self.fc1(x)\n",
        "#         return logit\n",
        "n_epochs = 5\n",
        "model = BiLSTMModel(hidden_size=128, num_layers=4, n_classes=3)\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "\n",
        "\n",
        "# Load train and test in Memory\n",
        "x_train = torch.tensor(train_X, dtype=torch.long)\n",
        "y_train = torch.tensor(train_y, dtype=torch.long)\n",
        "x_cv = torch.tensor(val_X, dtype=torch.long)\n",
        "y_cv = torch.tensor(val_y, dtype=torch.long)\n",
        "\n",
        "# Create Torch datasets\n",
        "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "valid = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
        "\n",
        "# Create Data Loaders\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start_time = time.time()\n",
        "    # Set model to train configuration\n",
        "    model.train()\n",
        "    avg_loss = 0.\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        # Predict/Forward Pass\n",
        "        y_pred = model(x_batch)\n",
        "        # Compute loss\n",
        "        loss = loss_fn(y_pred, y_batch.float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item() / len(train_loader)\n",
        "\n",
        "    # Set model to validation configuration -Doesn't get trained here\n",
        "    model.eval()\n",
        "    avg_val_loss = 0.\n",
        "    val_preds = np.zeros((len(x_cv),3))\n",
        "\n",
        "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
        "        y_pred = model(x_batch).detach()\n",
        "        avg_val_loss += loss_fn(y_pred, y_batch.float()).item() / len(valid_loader)\n",
        "        # keep/store predictions\n",
        "        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()\n",
        "\n",
        "    # Check Accuracy\n",
        "    val_accuracy = sum(val_preds.argmax(axis=1)==val_y.argmax(axis=1))/len(val_y)\n",
        "    train_loss.append(avg_loss)\n",
        "    valid_loss.append(avg_val_loss)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(\n",
        "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))\n",
        "dev_df = pd.read_csv(\"/content/test.csv\")\n",
        "dev_df = process_data(dev_df, 'sentence')\n",
        "test_X = tokenizer.texts_to_sequences(dev_df['sentence'].astype(str))\n",
        "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
        "x_cv = torch.tensor(test_X, dtype=torch.long)\n",
        "valid = torch.utils.data.TensorDataset(x_cv)\n",
        "valid_loader = torch.utils.data.DataLoader(valid, batch_size=64, shuffle=False)\n",
        "model.eval()\n",
        "y_hat=[]\n",
        "batch_size=64\n",
        "val_preds = np.zeros((len(x_cv),3))\n",
        "for i, (x_batch) in enumerate(valid_loader):\n",
        "        #print(x_batch[0])\n",
        "        y_pred = model(x_batch[0]).detach()\n",
        "        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()\n",
        "y_hat=(val_preds.argmax(axis=1))\n",
        "k=0\n",
        "with open('/content/answer.txt', 'w') as f:\n",
        "    for i in (y_hat-1):\n",
        "        f.write(str(i))\n",
        "        k+=1\n",
        "        if k==4855:\n",
        "          break\n",
        "        f.write('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncgijiG37ZQY"
      },
      "outputs": [],
      "source": [
        "len(dev_df)==4855"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
